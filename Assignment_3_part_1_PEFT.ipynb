{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3\n",
        "## Part 1: Finetuning with QLoRA\n",
        "\n",
        "In the Assignment 3 folder you'll find a notebook, `Class_9_ code_supplement_QLoRA_finetuning.ipynb`.  This provides you with all of the code that you need to perform PEFT on a quantized LLM.  Read through this notebook and make sure that you understand it.\n",
        "\n",
        "Run your own finetuning experiment to improve a base LLM's abilty to perform some task. Replace `davanstrien/haiku_prompts` with your own finetuning dataset. There are nunerous sources for such datasets including [Hugging Face](https://huggingface.co/datasets).  Remember that you're trying to *improve* the LLM's ability to perform a task so you may need to test the prompts from several datasets to see what the LLM currently struggles with.\n",
        "\n",
        "You also have the option of writing your own prompts for this task, using the format in `davanstrien/haiku_prompts`\n",
        "\n",
        "After you have finetuned your model answer the following questions:\n",
        "\n",
        "1. Provide before and after output showing the improvement in the model's performance on the task that you chose. If you see degradation instead of improvement in performance can you list a few reasons why this result occurred?\n",
        "\n",
        "\n",
        "2. Increase the value of the `r` paraemter in `LoraConfig` and, re-run the finetuning and then, as in Question 1, provide before and after examples of output, but this time, your \"before\" output should come from the model trained with a `r` of 16 and your \"after\" output should come from the model trained with an increased `r`\n",
        "\n",
        "If the output quality improved, theorize why this might be so, If the output degreded, also theorize why this might be so."
      ],
      "metadata": {
        "id": "E6FXBJ2pd88L"
      }
    }
  ]
}