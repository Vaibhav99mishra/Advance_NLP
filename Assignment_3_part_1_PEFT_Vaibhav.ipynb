{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3\n",
        "## Part 1: Finetuning with QLoRA\n",
        "\n",
        "In the Assignment 3 folder you'll find a notebook, `Class_9_ code_supplement_QLoRA_finetuning.ipynb`.  This provides you with all of the code that you need to perform PEFT on a quantized LLM.  Read through this notebook and make sure that you understand it.\n",
        "\n",
        "Run your own finetuning experiment to improve a base LLM's abilty to perform some task. Replace `davanstrien/haiku_prompts` with your own finetuning dataset. There are nunerous sources for such datasets including [Hugging Face](https://huggingface.co/datasets).  Remember that you're trying to *improve* the LLM's ability to perform a task so you may need to test the prompts from several datasets to see what the LLM currently struggles with.\n",
        "\n",
        "You also have the option of writing your own prompts for this task, using the format in `davanstrien/haiku_prompts`\n",
        "\n",
        "After you have finetuned your model answer the following questions:\n",
        "\n",
        "1. Provide before and after output showing the improvement in the model's performance on the task that you chose. If you see degradation instead of improvement in performance can you list a few reasons why this result occurred?\n",
        "\n",
        "\n",
        "2. Increase the value of the `r` paraemter in `LoraConfig` and, re-run the finetuning and then, as in Question 1, provide before and after examples of output, but this time, your \"before\" output should come from the model trained with a `r` of 16 and your \"after\" output should come from the model trained with an increased `r`\n",
        "\n",
        "If the output quality improved, theorize why this might be so, If the output degreded, also theorize why this might be so."
      ],
      "metadata": {
        "id": "E6FXBJ2pd88L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQF80m1wU8vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Provide before and after output showing the improvement in the model's performance on the task that you chose. If you see degradation instead of improvement in performance can you list a few reasons why this result occurred?\n",
        "\n",
        "\n",
        "Answer-\n",
        "\n",
        "Analysis of Current Results\n",
        "Output Comparison\n",
        "Base Model Output:\n",
        "\n",
        "The base model provides a lengthy and poetic output, which includes structured sentences and a somewhat meaningful flow.\n",
        "However, the text is repetitive and lacks cohesion, drifting away from the original prompt (\"Write a poem about the sea\").\n",
        "The content is verbose and includes irrelevant phrases, indicating it hasn’t been fine-tuned for poetic tasks.\n",
        "Fine-Tuned Model Output:\n",
        "\n",
        "The fine-tuned model generates output closer to poetic language but appears incoherent and grammatically inconsistent.\n",
        "The text shows fragmented ideas and lacks structure. It also introduces unrelated terms and concepts, which may indicate a training-related issue (e.g., overfitting or insufficient training data).\n",
        "Reasons for Degradation in Fine-Tuned Model Output\n",
        "Insufficient Data:\n",
        "\n",
        "The dataset may not be large or diverse enough to fine-tune the model effectively for poetry generation.\n",
        "Fine-tuning on a small dataset can lead to overfitting, causing the model to perform poorly on unseen prompts.\n",
        "Training Duration:\n",
        "\n",
        "Training for only 5 epochs may not have been sufficient for the model to learn meaningful representations from the dataset.\n",
        "Increasing the number of epochs or applying gradient accumulation could yield better results.\n",
        "LoRA Configuration:\n",
        "\n",
        "The r parameter in LoRA (rank of low-rank matrices) might have been too small (r = 4). A larger r allows more capacity for fine-tuning, which may be necessary for complex tasks like poetry generation.\n",
        "Learning Rate:\n",
        "\n",
        "The learning rate (2e-5) may not be optimal for this fine-tuning task. A lower rate (e.g., 1e-5) could stabilize training, especially when using LoRA.\n",
        "Prompt Ambiguity:\n",
        "\n",
        "The prompt \"Write a poem about the sea\" may not provide enough guidance. Using more explicit or structured prompts could help generate better outputs.\n",
        "\n",
        "\n",
        "\n",
        "finetuned model output keep r=4-\n",
        "\n",
        "Output from fine-tuned model: Write a poem about the sea  in', in thy prize. and bravely, i had been! but he stood before us!\"--the world? — my heart to read or else; nor was his own hands on all of peace: \"and morn’s her lover's eyes like that is so long ago you are always seem... well,' says it did not only dreamer'd beworship which man made by its branches round this spirit lives,\" said 'emps at hand through o'n't make no longer than yours alone she will liveliest enemy deadlift.\" -- what do they mettle feet high from below) whose life as an oxford shuddlesomey mountains boston spake up again?\" It would have learned how can find\n",
        "\n",
        "base model output-\n",
        "Output from base model: Write a poem about the sea\n",
        "From what you can glean from the ocean,\n",
        "The fish can eat in water.\n",
        "Let us live in a world of great danger.\n",
        "For a boat will be built on a spot.\n",
        "And when the sun will pass from the horizon.\n",
        "But when that dark sea turns\n",
        "From where the shore lies?\n",
        "And the waters turn into clouds.\n",
        "I am a true lover of peace.\n",
        "When my husband and I die\n",
        "My love will die out with him.\n",
        "You shall never regret it\n",
        "For no thing has so long taken my heart.\n",
        "And in my heart, my spirit remains high! I do keep your words of heart.\n",
        "Now, where I sit\n",
        "If the sun shines out\n",
        "In my presence\n",
        "Showing love, peace and love.\n",
        "O that I should speak\n",
        "And live\n",
        "This way.\n",
        "But do you still feel a little weary\n",
        "Of the world we live in?\n",
        "Of the world we live in?\n",
        "But you may feel a little comfort\n",
        "In what I have to do\n",
        "With the things I have to do.\n",
        "\"In your sleep are I able\n",
        "To carry you to heaven.\n",
        "\"I shall tell you one thing\n",
        "That is true for you.\"\n",
        "And I will return to you\n",
        "What God has sent and written for you.\n",
        "Love\n",
        "Mighty God does love me\n",
        "Love has no side, as the world itself says.\n",
        "I hold you in her arms\n",
        "With the most sacred vows,\n",
        "O love my friend.\n",
        "Love has no end, because I must\n",
        "Be free\n",
        "Of so much to do.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kFq-LxShVAUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Increase the value of the r paraemter in LoraConfig and, re-run the finetuning and then, as in Question 1, provide before and after examples of output, but this time, your \"before\" output should come from the model trained with a r of 16 and your \"after\" output should come from the model trained with an increased r\n",
        "\n",
        "\n",
        "#Results Analysis and Discussion\n",
        "Comparison of Outputs with Increased r Parameter\n",
        "\n",
        "# Before Output (r = 16):\n",
        "\n",
        "\n",
        "#Write a poem about the sea the-s, and then he had heard--\" by his back to thy brotherly eyes. in their lives of our home with her head; butler is that was not so you seein'er's marendi', i know\n",
        "\n",
        "#Strengths:\n",
        "Attempt at poetic language is evident.\n",
        "Incorporates some structure and descriptive phrases (e.g., \"by his back to thy brotherly eyes\").\n",
        "\n",
        "#Weaknesses:\n",
        "Sentence coherence is weak.\n",
        "The text includes nonsensical terms (\"seatin'er's marendi'\").\n",
        "Limited focus on the sea or related imagery, suggesting the model struggles to stay on topic.\n",
        "\n",
        "\n",
        "# After Output (r = 32):\n",
        "\n",
        "\n",
        "Write a poem about the seaThe road, and his eyes of youth's footsteps. she came hither; but forget him all that he who know how? -- to see thee! — i can scarce leaves us: 'the sun shines on my soul had nothilda-bl\n",
        "\n",
        "#Strengths:\n",
        "Slightly more coherent compared to r = 16.\n",
        "Includes some poetic constructs (e.g., \"the sun shines on my soul\").\n",
        "Better attempt at evoking emotions and imagery related to \"youth's footsteps\" and \"the sun.\"\n",
        "Weaknesses:\n",
        "Text still lacks clear thematic focus on the sea.\n",
        "Ends abruptly with incomplete words (\"nothilda-bl\").\n",
        "Inconsistent grammar and structure suggest the model is struggling to generalize effectively.\n",
        "Analysis of Results\n",
        "Increasing the r parameter from 16 to 32 provided some improvement:\n",
        "\n",
        "#Coherence: The model trained with r = 32 shows slightly better sentence flow and poetic phrasing.\n",
        "#Imagery: The output includes more vivid imagery (e.g., \"the sun shines on my soul\").\n",
        "#Focus: Despite some improvement, the model still fails to stay consistently focused on the prompt theme (\"the sea\").\n",
        "\n",
        "\n",
        "## Potential Reasons for Observed Results\n",
        "#Insufficient Data: The dataset may not contain enough examples of poems specific to the \"sea\" or related themes, limiting the model's ability to generalize.\n",
        "#LoRA Configuration: While increasing r improves capacity, it may still be insufficient for capturing the nuances required for this task. Further increases in r (e.g., r = 64) could improve results but may lead to overfitting.\n",
        "#Training Duration: Training for only 10 epochs may not have allowed the model to fully adapt to the dataset. Increasing epochs could improve performance.\n",
        "#Learning Rate: Fine-tuning with a slightly lower learning rate (e.g., 1e-5 to 5e-6) might stabilize training and lead to better generalization."
      ],
      "metadata": {
        "id": "rDbOTq0zWTJK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kiuKLSPqZKPb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}